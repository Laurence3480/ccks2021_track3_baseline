基于 bert4keras 的一个baseline

不作任何 数据trick 单模 线上 最高可到 0.7891


# 基础 版
train.py         0.7769

# transformer 各层 cls concat   明神的trick https://xv44586.github.io/2021/01/20/ccf-qa-2/
train_concat.py  0.7786

# 使用那个FGM对抗
train_FGM.py     0.7891

# 苏神提出的 PET
train_PET.py     0.7812


预训练模型使用 nezha

使用说明：
	数据放在 data下
	下载预训练模型解压
	模型会保存在 train中

优化方向：
 1，融合🙃

bert4keras >= 0.10.0

nezha的预训练模型
链接: https://pan.baidu.com/s/1lURyXs39PYnsb4imCjVkfQ  密码: 1eqq
--来自百度网盘超级会员V4的分享